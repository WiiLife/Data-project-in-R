---
title: "Report draft"
output: html_document
date: "2025-05-20"
---

# Data Projects & Hackathon

## Project: The coool project

### Authors: [William Ambrosetti](https://github.com/WiiLife), [Alexandra Biddiscombe](https://github.com/ambiddisco), [Youssef Sedra](https://github.com/ysedra)

## 1. Abstract

to be completed at a later date. Should be a brief synopsis of the main points discovered during research.

## 2. Introduction

Every moment, people make decisions about their daily lives, based on information from all around them. This information may come from a news article, a video on their phone, something said to them by a friend, or even official statistics from the government. There are in fact so many sources of information that it can be overwhelming to think about. In other words, there is tons of data all around us.

As students of data science, we are expected to look at this enormous amount of data and draw a few logical conclusions that can help the everyday person with their quality of life. Some day, we will be posed the question to analyse data from a topic we never even knew existed and make important decisions with it. The idea may seem alien to us right now, but it is a reality of our chosen profession.

To embody this strange feeling, we decided to take the metaphor literally and make ourselves into the most confused data scientists tasked with helping humanity: Aliens.

For the duration of this report, we will be using a select amount of data to draw relevant conclusions about the state of the our world, and more specifically, based around the topic of health and medicine, as minimally informed but well-meaning "aliens". We will base our exploration and experiments on generic data and attempt to advise different countries on how to improve their overall well-being, using metrics we will discuss later.

## 3. Data Exploration

### 3.1 Research questions

We are looking at this data project as aliens who are very logical but not very informed, and have a slice of data with which to assess the world. We know there are many different countries, that are separated by borders and each decide how to rule their own territories. We want to determine the best countries, using information inherent to the medical scope, and their primary features of influence. This will help us find ways to improve and suggest improvements to countries doing less well.

We would like to find out:

-   What are the best outcomes for a country, and what are the primary influencing features for these outcomes;
-   Using the most influential features found above, can we determine a given country's continent of origin
-   As a country that is not doing well, which features should it improve;
-   binary calssification (phrase better)

### 3.2 Data cleaning

The datasets we chose represent data from the WHO, found on [Kaggle](https://kaggle.com) at [World Health Statistics 2020\|Complete\|Geo-Analysis](https://www.kaggle.com/datasets/utkarshxy/who-worldhealth-statistics-2020-complete), curated by [Zeus](https://www.kaggle.com/utkarshxy).

The data we chose is separated into many different files, each of which only contains only one or two different features describing the state of the world, divided by country and by year. To use data like this, we need to determine which are the most relevant features for our research questions and how to evaluate them.

Our first step is to separate the features into variables we can use to evaluate the state of a country (outcome variables) and variables which influence the state of a country (decision variables). As an example, let's use the idea of getting to work on time: the outcome of "being on time" or "being late" depends on the decision variable "time of leaving the house". The decision to leave earlier may allow you to arrive on time to work, even if there are other variables that also influence the outcome like the presence of traffic.

```{r}
# Packages
library(readr)
library(dplyr)
library(tidyverse)
library(tidymodels)
library(GGally)
library(xgboost)
library(glmnet)
library(ranger)
options(ranger.num.threads = 16)
library(kknn)

```

```{r}
#Importing the data, one feature dataframe at a time

# Data

# Outcome features
infant_mortality_rate_data = read_csv("data/infantMortalityRate.csv")
maternal_mortality_ratio_data = read_csv("data/maternalMortalityRatio.csv")
life_expectency_at_birth_data = read_csv("data/lifeExpectancyAtBirth.csv")
under_5_mortality_rate = read_csv("data/under5MortalityRate.csv")
incidence_of_malaria_data = read_csv("data/incedenceOfMalaria.csv")
incidence_of_tuberculosis_data = read_csv("data/incedenceOfTuberculosis.csv")
new_hiv_infections = read_csv("data/newHivInfections.csv")
neo_natal_mortality_rate = read_csv("data/neonatalMortalityRate.csv")
crude_sucide_rates = read_csv("data/crudeSuicideRates.csv") # has to do with well-being as it reflects happiness
poison_mortality_rate = read_csv("data/mortalityRatePoisoning.csv") # consider removing, not altogether medical related
road_traffic_deaths = read_csv("data/roadTrafficDeaths.csv") # consider removing, not medical related

# Decision features
medical_doctors_data = read_csv("data/medicalDoctors.csv")
pharmacists_data = read_csv("data/pharmacists.csv")
birth_by_skilled_personel_data = read_csv("data/birthAttendedBySkilledPersonal.csv")
number_of_dentists = read_csv("data/dentists.csv")
drinking_water_services = read_csv("data/basicDrinkingWaterServices.csv")
least_basic_sanitation_services = read_csv("data/atLeastBasicSanitizationServices.csv")
basic_hand_washing = read_csv("data/basicHandWashing.csv")
nursing_services = read_csv("data/nursingAndMidwife.csv")
sanitation_services = read_csv("data/safelySanitization.csv")
clean_fuel_and_teck = read_csv("data/cleanFuelAndTech.csv") # consider removing, not medical related
tabacco_age_15 = read_csv("data/tobaccoAge15.csv") # consider removing, not medical related

# Note: many features correlate on a given model or prediction, so in many cases 
# we will not use the totality of the data as the information in a given features 
# may not be unique, and is found in other features for a specific use case.

# Test one data set
head(medical_doctors_data, 5)
```

After loading the data, some cleaning is necessary before we can jump into exploration. We created a larger merged dataframe (tibble) to contain all the data we plan on using, and removed features with too little information or that are unsuitable.

```{r}
# function to remove indicator and rename `First Tooltip` to indicator feature name
replace_inidcator_to_tooltip = function(tibble) {
  df = tibble
  feat_name = tibble$Indicator[1]
  df = subset(df, select = - Indicator)
  names(df)[names(df) == "First Tooltip"] <- feat_name
  
  return(df)
}
```

```{r}

# Manual overview of the data shows that
# lowest year is 1962 
# highest year is 2019

# Creating an empty merged_data so we can add all the features from the datasets:

# Manual overview of data reveals the following countries. 
# Putting in a list to be used for table creation.
countries = c(
    "Afghanistan", "Albania", "Algeria", "Angola", "Antigua and Barbuda",
    "Argentina", "Armenia", "Australia", "Austria", "Azerbaijan",
    "Bahamas", "Bahrain", "Bangladesh", "Barbados", "Belarus",
    "Belgium", "Belize", "Benin", "Bhutan", "Bolivia (Plurinational State of)",
    "Bosnia and Herzegovina", "Botswana", "Brazil", "Brunei Darussalam",
    "Bulgaria", "Burkina Faso", "Burundi", "Cabo Verde", "Cambodia",
    "Cameroon", "Canada", "Central African Republic", "Chad", "Chile",
    "China", "Colombia", "Comoros", "Congo", "Costa Rica", "Croatia",
    "Cuba", "Cyprus", "Czechia", "CÃ´te d'Ivoire", "Democratic People's Republic of Korea",
    "Democratic Republic of the Congo", "Denmark", "Djibouti",
    "Dominican Republic", "Ecuador", "Egypt", "El Salvador", "Equatorial Guinea",
    "Eritrea", "Estonia", "Eswatini", "Ethiopia", "Fiji", "Finland",
    "France", "Gabon", "Gambia", "Georgia", "Germany", "Ghana",
    "Greece", "Grenada", "Guatemala", "Guinea", "Guinea-Bissau",
    "Guyana", "Haiti", "Honduras", "Hungary", "Iceland", "India",
    "Indonesia", "Iran (Islamic Republic of)", "Iraq", "Ireland",
    "Israel", "Italy", "Jamaica", "Japan", "Jordan", "Kazakhstan",
    "Kenya", "Kiribati", "Kuwait", "Kyrgyzstan", "Lao People's Democratic Republic",
    "Latvia", "Lebanon", "Lesotho", "Liberia", "Libya", "Lithuania",
    "Luxembourg", "Madagascar", "Malawi", "Malaysia", "Maldives",
    "Mali", "Malta", "Mauritania", "Mauritius", "Mexico", "Micronesia (Federated States of)",
    "Mongolia", "Montenegro", "Morocco", "Mozambique", "Myanmar",
    "Namibia", "Nepal", "Netherlands", "New Zealand", "Nicaragua",
    "Niger", "Nigeria", "Norway", "Oman", "Pakistan", "Panama",
    "Papua New Guinea", "Paraguay", "Peru", "Philippines", "Poland",
    "Portugal", "Qatar", "Republic of Korea", "Republic of Moldova",
    "Romania", "Russian Federation", "Rwanda", "Saint Lucia",
    "Saint Vincent and the Grenadines", "Samoa", "Sao Tome and Principe",
    "Saudi Arabia", "Senegal", "Serbia", "Seychelles", "Sierra Leone",
    "Singapore", "Slovakia", "Slovenia", "Solomon Islands", "Somalia",
    "South Africa", "South Sudan", "Spain", "Sri Lanka", "Sudan",
    "Sudan (until 2011)", "Suriname", "Sweden", "Switzerland",
    "Syrian Arab Republic", "Tajikistan", "Thailand", "The former Yugoslav Republic of Macedonia",
    "Timor-Leste", "Togo", "Tonga", "Trinidad and Tobago", "Tunisia",
    "Turkey", "Turkmenistan", "Uganda", "Ukraine", "United Arab Emirates",
    "United Kingdom of Great Britain and Northern Ireland", "United Republic of Tanzania",
    "United States of America", "Uruguay", "Uzbekistan", "Vanuatu",
    "Venezuela (Bolivarian Republic of)", "Viet Nam", "Yemen",
    "Zambia", "Zimbabwe", "Andorra", "Cook Islands", "Dominica",
    "Marshall Islands", "Nauru", "Niue", "Palau", "Saint Kitts and Nevis",
    "State of Palestine", "Tuvalu", "Monaco", "San Marino", "Germany, Federal Republic (former)",
    "India (until 1975)", "Kiribati (until 1984)", "South Viet Nam (former)")

years <- 2019:1962

merged_data = tibble(Location=countries) |> mutate(Period = list(years)) |> unnest(Period)

merged_data
```

```{r}
# Pipelining each data set through data modifications it needs to become tidy
medical_doctors_data = medical_doctors_data |> replace_inidcator_to_tooltip()
birth_by_skilled_personel_data = birth_by_skilled_personel_data |> replace_inidcator_to_tooltip()
life_expectency_at_birth_data = life_expectency_at_birth_data |> replace_inidcator_to_tooltip() |> pivot_wider(names_from = Dim1, values_from=`Life expectancy at birth (years)`)
pharmacists_data = pharmacists_data |> replace_inidcator_to_tooltip()
infant_mortality_rate_data = infant_mortality_rate_data |> replace_inidcator_to_tooltip() |> pivot_wider(names_from = Dim1, values_from = `Infant mortality rate (probability of dying between birth and age 1 per 1000 live births)`) |> mutate(across(c(`Both sexes`, Male, Female), ~ as.numeric(sub(" .*", "", .x))))
maternal_mortality_ratio_data = maternal_mortality_ratio_data |> replace_inidcator_to_tooltip() |> mutate(across(`Maternal mortality ratio (per 100 000 live births)`, ~ as.numeric(sub(" .*", "", .x))))
incidence_of_malaria_data = incidence_of_malaria_data |> replace_inidcator_to_tooltip()
incidence_of_tuberculosis_data = incidence_of_tuberculosis_data |> replace_inidcator_to_tooltip() |> mutate(across(`Incidence of tuberculosis (per 100 000 population per year)`, ~ as.numeric(sub(" .*", "", .x))))

# air_pollution_death_rate = air_pollution_death_rate |> mutate(across(`First Tooltip`, ~ as.numeric(sub(" .*", "", .x)))) |> pivot_wider(names_from = c(Dim1, Dim2), values_from = `First Tooltip`) |> replace_inidcator_to_tooltip()

crude_sucide_rates = crude_sucide_rates |> pivot_wider(names_from=Dim1, values_from=`First Tooltip`) |> replace_inidcator_to_tooltip()
poison_mortality_rate = poison_mortality_rate |> pivot_wider(names_from=Dim1, values_from=`First Tooltip`) |> replace_inidcator_to_tooltip()
neo_natal_mortality_rate = neo_natal_mortality_rate |> mutate(across(`First Tooltip`, ~ as.numeric(sub(" .*", "", .x)))) |> replace_inidcator_to_tooltip() |> select(-c(Dim1))
new_hiv_infections = new_hiv_infections |> drop_na() |> mutate(across(`First Tooltip`, ~ as.numeric(sub(" .*", "", .x)))) |> pivot_wider(names_from=Dim1, values_from=`First Tooltip`) |> replace_inidcator_to_tooltip()
road_traffic_deaths = road_traffic_deaths |> replace_inidcator_to_tooltip()
under_5_mortality_rate = under_5_mortality_rate |> mutate(across(`First Tooltip`, ~ as.numeric(sub(" .*", "", .x)))) |> pivot_wider(names_from=Dim1, values_from=`First Tooltip`) |> replace_inidcator_to_tooltip()
number_of_dentists = number_of_dentists |> replace_inidcator_to_tooltip()
drinking_water_services = drinking_water_services |> replace_inidcator_to_tooltip()
least_basic_sanitation_services = least_basic_sanitation_services |> pivot_wider(names_from=Dim1, values_from=`First Tooltip`) |> replace_inidcator_to_tooltip()
basic_hand_washing = basic_hand_washing |> pivot_wider(names_from=Dim1, values_from=`First Tooltip`) |> replace_inidcator_to_tooltip()
clean_fuel_and_teck = clean_fuel_and_teck |> replace_inidcator_to_tooltip()
tabacco_age_15 = tabacco_age_15  |> pivot_wider(names_from=Dim1, values_from=`First Tooltip`) |> replace_inidcator_to_tooltip()
nursing_services = nursing_services |> replace_inidcator_to_tooltip()
sanitation_services = sanitation_services |> pivot_wider(names_from=Dim1, values_from=`First Tooltip`) |> replace_inidcator_to_tooltip()

```

```{r}
# Merging
merged_data = full_join(merged_data, drinking_water_services, by=c("Location", "Period"))
merged_data = full_join(merged_data, number_of_dentists, by=c("Location", "Period"))
merged_data = full_join(merged_data, least_basic_sanitation_services |> rename(sanitation_services=Total) |> select(-c(Urban, Rural)), by=c("Location", "Period"))
merged_data = full_join(merged_data, basic_hand_washing |> rename(basic_hand_washing_services=Total) |> select(-c(Urban, Rural)), by=c("Location", "Period"))
merged_data = full_join(merged_data, tabacco_age_15 |> rename(tabacco_age_15=`Both sexes`) |> select(-c(Male, Female)), by=c("Location", "Period"))
merged_data = full_join(merged_data, nursing_services, by=c("Location", "Period"))
merged_data = full_join(merged_data, sanitation_services |> rename(total_sanitation_services=Total) |> select(-c(Urban, Rural)), by=c("Location", "Period"))
merged_data = full_join(merged_data, under_5_mortality_rate |> rename(under_5_mortality_rate=`Both sexes`) |> select(-c(Male, Female)), by=c("Location", "Period"))
merged_data = full_join(merged_data, road_traffic_deaths |> mutate(`Estimated road traffic death rate (per 10 000 population)`=`Estimated road traffic death rate (per 100 000 population)`/10) |> select(-c(`Estimated road traffic death rate (per 100 000 population)`)), by=c("Location", "Period"))
merged_data = full_join(merged_data, new_hiv_infections |> rename(hiv_infections=`Both sexes`) |> select(-c(Male, Female)), by=c("Location", "Period"))
merged_data = full_join(merged_data, neo_natal_mortality_rate |> mutate(`Neonatal mortality rate (per 10 000 live births)`=`Neonatal mortality rate (per 1000 live births)`*10) |> select(-c(`Neonatal mortality rate (per 1000 live births)`)), by=c("Location", "Period"))
merged_data = full_join(merged_data, poison_mortality_rate |> rename(poison_mortality_rate=`Both sexes`) |> select(-c(Male, Female)), by=c("Location", "Period"))
merged_data = full_join(merged_data, crude_sucide_rates |> rename(sucide_rate=`Both sexes`) |> select(-c(Male, Female)), by=c("Location", "Period"))
merged_data = full_join(merged_data, life_expectency_at_birth_data |> rename(life_expectency_at_birth=`Both sexes`) |> select(-c(Male, Female)), by=c("Location", "Period"))
merged_data = full_join(merged_data, birth_by_skilled_personel_data, by=c("Location", "Period"))
merged_data = full_join(merged_data, medical_doctors_data, by=c("Location", "Period"))
merged_data = full_join(merged_data, pharmacists_data, by=c("Location", "Period"))
merged_data = full_join(merged_data, infant_mortality_rate_data |> rename(infant_mortality_rate=`Both sexes`) |> select(-c(Male, Female)), by=c("Location", "Period"))
merged_data = full_join(merged_data, maternal_mortality_ratio_data |> mutate(`Maternal mortality ratio (per 10 000 live births)`=`Maternal mortality ratio (per 100 000 live births)`*10) |> select(-c(`Maternal mortality ratio (per 100 000 live births)`)), by=c("Location", "Period"))
merged_data = full_join(merged_data, incidence_of_malaria_data |> mutate(`Malaria incidence (per 10 000 population at risk)`=`Malaria incidence (per 1 000 population at risk)`*10) |> select(-c(`Malaria incidence (per 1 000 population at risk)`)), by=c("Location", "Period"))
merged_data = full_join(merged_data, incidence_of_tuberculosis_data |> mutate(`Incidence of tuberculosis (per 10 000 population per year)`=`Incidence of tuberculosis (per 100 000 population per year)`*10) |> select(-c(`Incidence of tuberculosis (per 100 000 population per year)`)), by=c("Location", "Period"))

merged_data

```

Note: many features correlate on a given model or prediction, so in many cases we will not use the totality of the data as the information in a given features may not be unique, and is found in other features for a specific use case.

To make every research question meaningful, all data sets used to find experiments and answers belong to the same time periods. We will select relevant features and time periods appropriately.

## 4. Experiments and findings

### 4.0 Preliminary data exploration

Using our invented setting, that we are benevolent aliens preparing to help some of Earth's countries better the quality of life of their people, we need to start by informing ourselves of Earth's situation. Looking first at the bigger picture we will have a better idea of how to approach the more specific questions. We understand that each country has its own borders and can decide for itself how to operate inside of those borders, so we need to know in which ways they are similar and in which they are different.

Knowing that we want to compare each country's outcome features and their decision or explanatory features, we start our explorations phase by comparing outcome features. we build a correlation matrix and look at the overall data, just to give ourselves an idea of where to start looking for the optimal features of a country.

```{r}
outcome_feat = c("life_expectency_at_birth",
                 "Maternal mortality ratio (per 10 000 live births)", 
                 "Neonatal mortality rate (per 10 000 live births)", 
                 "infant_mortality_rate",
                 "under_5_mortality_rate", 
                 "Malaria incidence (per 10 000 population at risk)", 
                 "Incidence of tuberculosis (per 10 000 population per year)", 
                 "hiv_infections", 
                 "sucide_rate", 
                 "poison_mortality_rate",
                 "Estimated road traffic death rate (per 10 000 population)")

explanatory_feat = setdiff(colnames(merged_data), outcome_feat)
explanatory_feat = setdiff(explanatory_feat, c("Location", "Period"))

# visualise outcome features of all data
merged_data[outcome_feat]

# correlate the outcome features
cor_matrix <- cor(merged_data[outcome_feat], use = "pairwise.complete.obs")
cor_matrix

# Show correlation matrix
ggpairs(cor_matrix, progress=FALSE)

```

We can notice some initial trends in the data, where many of these outcomes are correlated to each other in significant ways. For instance, let's look at the first column, which contains data about the life expectancy at birth: it is very correlated to all the other outcomes, in fact 5 correlations show a coefficient of 0.9 or above.

Since many outcomes are correlated we are led to guess that these are highly influenced by the explanatory features, and even if we remove some outcome features we will be able to accurately predict which countries are performing better (well-being wise) based on those we keep.

Having said these, we move onto a second part of data cleaning: if we want to provide usable and reality-based answers to our research questions, we need the data to be a good representation of real life, but also for it to be consistent and clean. For this reason, we explore the missing values and usable data.

At first we will decide what to keep based on data availability, by removing rows or columns (entries or features) with too many non existent values.

Let's visualise the number of missing datapoints by year, and then by feature, to obtain a more usable set of data.

```{r}
# More data cleaning
non_na_per_period <- merged_data %>% select(-c(Location)) %>%
  rowwise() %>%
  mutate(non_na_count = sum(!is.na(c_across(where(~ !is.list(.x)))))) %>%
  ungroup() %>%
  group_by(Period) %>%
  summarise(total_non_na = sum(non_na_count))

# Here we plot number of not NA values by year of the data
ggplot(non_na_per_period, aes(x = Period, y = total_non_na)) +
  geom_col(fill = "seagreen") +
  labs(title = "Total Non-NA Values per Period",
       x = "Year",
       y = "Non-NA Value Count") +
  theme_minimal()

# From the outputs we can see there is a lot more data available after the year 2000, and the data before would be difficult to use.
# Lets keep the data only from 2000 onward
merged_data = merged_data |> filter(Period >= 2000)

# Here we plot the count of NA values per feature, for the segment of data we decided to keep 
na_counts <- merged_data %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "feature", values_to = "na_count")

ggplot(na_counts, aes(x = reorder(feature, na_count), y = na_count)) +
  geom_col(fill = "tomato") +
  coord_flip() +
  labs(title = "NA Values per Feature",
       x = "Feature",
       y = "NA Count") +
  theme_minimal()

# We set an arbitrary limit to the number of NA values per feature at 3000, so as to remove the most difficult cases
# but not remove too much of the data
valid_features <- merged_data %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "feature", values_to = "na_count") %>%
  filter(na_count < 3000) %>%
  pull(feature)

# Finally creating the new cleaned data 
cleaned_merged_data <- merged_data %>%
  select(all_of(valid_features))

```

```{r}

```

We decide to keep only the years for which we have the most data, and the same logic applies to the features. We set an arbitrary value for number of missing values to accept for the features, set at 3000 so as to remove the ones with the most missing values, but still keep enough of the original features to make interesting explorations with.

Of the original 11 outcome variables we end up keeping 6. We are OK with this because as we discussed earlier, the outcomes are correlated enough with each other to still give a decent overview.

### 4.1 Question 1

#### What are the best outcomes for a country, and what are the primary influencing features for those outcomes?

Our first research question surrounds the general state of the countries in our data set. Can we determine what the best outcome for a country is and how it is affected by the available decision variables?

First of all we need to decide what is "the best" outcome. This could be represented by the least mortality, the highest life expectancy at birth, the least amount of suicides or many other things. To find an overall score for what could be the best, we selected the a recent year in the dataframe for which we have a good amount of data, and looked for that year's happiness index.

Since we have cleaned the data since doing the preliminary data exploration, let's look at the new outcome correlations.

```{r}
# new outcome and explanatory features
explanatory_feat = intersect(explanatory_feat, names(cleaned_merged_data))
outcome_feat = intersect(outcome_feat, names(cleaned_merged_data))

# visualise outcome features of all data
cleaned_merged_data[outcome_feat]

# correlate the outcome features
cleaned_cor_matrix <- cor(cleaned_merged_data[outcome_feat], use = "pairwise.complete.obs")

# Show correlation matrix
ggpairs(cleaned_cor_matrix, progress=FALSE)
```

Next let's select the happiest countries for a given year. Using the amount of missing data per each year, we select a suitable candidate which is also preferably close in time to the current date of May 2025, and visualise the top 10 countries by happiness of that year.

```{r}
for (year in 2000:2019) {
  print(paste(year, sum(is.na(filter(cleaned_merged_data, Period == year)))))
}

```

A good date to explore is 2015, although there are fewer missing values in 2010, 2015 is closer to the actual date.

The top 10 countries by happiness index were recovered from [the World Happiness Report data](https://data.worldhappiness.report/table).

```{r}
happy_countries_list <- c("Norway", "Switzerland", "Denmark", "Iceland", "Finland","Canada", "Netherlands", "New Zealand", "Australia", "Sweden")

# for (year in 2000:2019) {
#   print(paste(year, sum(is.na(filter(merged_data, Period == year, Location %in% happy_countries_list)))))}

# 2015 still a good value for the happy country data

happy_countries_2015 <- filter(cleaned_merged_data, Period == 2015, Location %in% happy_countries_list) 
happy_countries_2015

# Let's look at a random feature
select(happy_countries_2015, "Location", "infant_mortality_rate")

```

Having found the happiest countries for a given date, let's compare the outcomes of the top 10 countries in 2015 to the outcomes of the other countries. Since the happiness index is calculated in the same way no matter the year, this should give us the idea of what statistics to look out for overall.

```{r}
cleaned_countries_2015 <- filter(cleaned_merged_data, Period == 2015)
countries_2015

```

```{r}

# bar plot of every country's outcome features 

 # plot every country feat 
#   make it into barplot (horizontal)
  
#  colour any coutry with value between min and max
  
  
# -> best outcome variable is infant mortality

country_col <- "Location"  # Replace with your actual country name column

for (feat in outcome_feat) {
  cat("Plotting:", feat, "\n")

  # Skip if all values are NA
  if (all(is.na(happy_countries_2015[[feat]]))) next

  # Extract feature values and remove NA rows
  df_feat <- cleaned_countries_2015[, c(country_col, feat)]
  df_feat <- df_feat[!is.na(df_feat[[feat]]), ]

  # Define min and max
  min_value <- min(happy_countries_2015[, c(country_col, feat)][[feat]])
  max_value <- max(happy_countries_2015[, c(country_col, feat)][[feat]])

  # Add highlight column
  df_feat$highlight <- ifelse(df_feat[[feat]] > min_value & df_feat[[feat]] < max_value, "In Range", "Min/Max")

  # Reorder factor levels for plotting
  df_feat[[country_col]] <- factor(df_feat[[country_col]], levels = df_feat[[country_col]][order(df_feat[[feat]])])

  # Plot using tidy evaluation
  p <- ggplot(df_feat, aes(x = .data[[country_col]], y = .data[[feat]], fill = highlight)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    scale_fill_manual(values = c("In Range" = "skyblue", "Min/Max" = "gray")) +
    labs(title = paste("Feature:", feat),
         x = "Country", y = feat) +
    theme_minimal()

  print(p)
}


```


```{r}


# Sample data
data <- data.frame(
  category = c("A", "B", "C", "D"),
  value = c(10, 15, 7, 20)
)

data

# Create the plot
ggplot(data, aes(x = category, y = value)) +
  # Add shaded area
  geom_rect(aes(xmin = as.numeric(category) - 0.5, xmax = as.numeric(category) + 0.5,
                ymin = 0, ymax = value), fill = "lightblue", alpha = 0.5) +
  # Add bars
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  labs(title = "Bar Plot with Shaded Area", x = "Category", y = "Value")

```



```{r}

ggpairs(cleaned_merged_data |> select(`Dentists (per 10,000)`, `Pharmacists  (per 10,000)`, `Medical doctors (per 10,000)`) |> drop_na(), progress = F)

```

Next we very generically look at the correlations between outcome and decision features. This will allow us to ignore certain features in the future, because the more correlated features are, the less new information they are giving us.

Let's compare the outcome feature values of the happiest countries in 2015 to the other countries.

```{r}
countries_2015 <- filter(merged_data, Period == 2015)
countries_2015

outcome_feat = c("life_expectency_at_birth",
                 "Maternal mortality ratio (per 10 000 live births)", 
                 "Neonatal mortality rate (per 10 000 live births)", 
                 "infant_mortality_rate",
                 "under_5_mortality_rate", 
                 #"Malaria incidence (per 10 000 population at risk)", 
                 "Incidence of tuberculosis (per 10 000 population per year)", 
                 "hiv_infections", 
                 "sucide_rate", 
                 "poison_mortality_rate")
                 #"Estimated road traffic death rate (per 10 000 population)")



explanatory_feat = setdiff(colnames(merged_data), outcome_feat)
explanatory_feat = setdiff(explanatory_feat, c("Location", "Period"))

happy_countries[outcome_feat]


# pot bars with values for all countries
# highlight area in which happy countries are

# correlation between outcomes and explanatory features

# idea: infant mortality and everyting else


# outcomes corr outcomes
drop_na(happy_countries[outcome_feat])

cor_matrix <- cor(happy_countries[outcome_feat], use = "pairwise.complete.obs")
drop_na(cor_matrix)
cor_matrix
# Show correlation matrix
ggpairs(cor_matrix, progress=FALSE)

```

```{r}
cleaned_countries_2015 <- filter(merged_data_clean, Period == 2015)

cleaned_merged_data


outcome_feat_clean = c("Maternal mortality ratio (per 10 000 live births)", 
                      "Neonatal mortality rate (per 10 000 live births)", 
                      "infant_mortality_rate",
                      "under_5_mortality_rate")

happy_countries_cleaned[outcome_feat_clean]

cor_matrix_clean <- cor(happy_countries_cleaned[outcome_feat_clean], use = "pairwise.complete.obs")
cor_matrix_clean <- cor(cleaned_merged_data[outcome_feat_clean], use = "pairwise.complete.obs")
cor_matrix_clean

ggpairs(cor_matrix_clean, progress=FALSE)

library(correlation)
correlation(cor_matrix_clean)

cleaned_merged_data[outcome_feat_clean]
```

## 5. Conclusion

The data still has many features that are interesting and can be related to each other. As a first glance at the world's well-being, we start by exploring a topic through similar features, to look for correlations and interesting information to note.

```{r}
# Features to select
birth_features <- c("Location", 
                    "Period", 
                    "Nursing and midwifery personnel (per 10,000)", 
                    "Births attended by skilled health personnel (%)",
                    "under_5_mortality_rate", 
                    "infant_mortality_rate",
                    "Neonatal mortality rate (per 10 000 live births)",
                    "Maternal mortality ratio (per 10 000 live births)")
birth_data <- select(cleaned_merged_data, birth_features)
birth_data
# Select only some years
birth_data <- filter(birth_data, Period %in% 2000:2019)

# Create the correlation matrix
cor_matrix <- cor(birth_data[,3:8], use = "pairwise.complete.obs")

# Show correlation matrix
ggpairs(cor_matrix, progress=FALSE) # , method="spearman"

```

From the correlation plot over the birth related medical features, we notice that there are a lot of clusters of data, and some very high correlations. When there are high correlations this leads us to believe the features may be related. We will explore this in one of our research questions.

There are many missing values throughout the data. To combat this, we use different techniques throughout the project, trying to find an accurate representation of reality.

For this first look into the data, regarding child birth and mortality, we will instead use a simple method, by dividing the data into bins spanning a 5 year period and averaging the country's value for each feature across each bin.

```{r}
# Check how many null values in each column
for (i in 1:length(birth_data)) {
  print(sum(is.na(birth_data[,i])) / nrow(birth_data))} 
# some columns have more than 50% missing values

# we try: Bin the years

# Create a new column that groups years into 5-year intervals
binned_birth_data <- birth_data %>%
  mutate(YearGroup = paste(floor((Period - min(Period)) / 5) * 5 + min(Period), "-", floor((Period - min(Period)) / 5) * 5 + min(Period) + 4))

# group the tibble by the countries and the year bin, such that each feature value is the average over the years of that bin
binned_birth_data <- binned_birth_data %>%
  group_by(Location, YearGroup) %>%
  summarise(across(where(is.numeric), ~mean(.x, na.rm = TRUE))) %>%
  select(!Period) # also remove the year

# replace "NaN" with NA, null value for R
binned_birth_data[binned_birth_data == "NaN"] <- NA
binned_birth_data

# Check if this helped with null values frequency
for (i in 1:length(binned_birth_data)) {
  print(sum(is.na(binned_birth_data[,i])) / nrow(binned_birth_data))} 
# Now there is a lower frequency of missing values

# Create the correlation matrix
cor_matrix <- cor(binned_birth_data[,3:8], use = "pairwise.complete.obs")

# Show correlation matrix
ggpairs(cor_matrix, progress=FALSE)

```

Combating null values in this case did not give a more or less accurate model, so we will stick to the first correlation matrix to draw conclusions.
